[page 19]
animals = ["dog", "cat", "fish"]
RDDFromMem = sc.parallelize(animals)

RDDFromFile = sc.textFile("hdfs:///tmp/sample.txt")

RDDFromAnotherRDD = RDDFromFile.map(lambda line: line.split(","))


[page 33]
fileRDD = sc.textFile("hdfs:///tmp/sample.txt")
RDD1 = fileRDD.map(lambda element: element.split("-"))
RDD1.take(3)


[page 34]
RDD2 = fileRDD.flatMap(lambda element: element.split("-"))
RDD2.take(15)


[page 35]
RDD3 = RDD2.distinct()
RDD3.take(15)


[page 36]
RDD4 = RDD3.filter(lambda word: word != "Tainan")
RDD4.take(15)


[page 37]
unionRDD = RDD2.union(RDD3)
unionRDD.take(30)


[page 39]
pairRDD = RDD2.map(lambda word: (word[0], word))
RDD5 = pairRDD.groupByKey()
RDD5.take(15)


[page 40]
RDD6 = pairRDD.sortByKey()
RDD6.take(15)


[page 41]
RDD7 = pairRDD.sortBy(lambda element: element[1][-1], False)
RDD7.take(15)


[page 47]
pairRDD1 = RDD2.map(lambda element: (element,1))
RDD8 = pairRDD1.reduceByKey(lambda value_x, value_y: value_x + value_y)
RDD8.take(15)


[page 48]
ageRDD = RDD1.map(lambda items: (items[0], (items[1], items[2])))

nationFileRDD = sc.textFile("hdfs:///tmp/nation.txt")
nationRDD = nationFileRDD.map(lambda line: line.split("-"))\
.map(lambda items: (items[0], (items[1], items[2])))
joinRDD = ageRDD.join(nationRDD)
joinRDD.take(15)


[page 73]
$ pyspark
songs_df = spark.sql ("select * from songs")
songs_df.show()

$ spark-submit spark-data/hive_query.py


[page 74]
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
my_schema = StructType(
[StructField("song_id",     StringType()),
StructField("song_length", IntegerType()),
StructField("genre_ids",   StringType()),
StructField("artist_name", StringType()),
StructField("composer",    StringType()),
StructField("lyricist",    StringType()),
StructField("language",    StringType())]
)

filePath = "/tmp/songs.csv"
songsDF = spark.read.format("csv")\
.schema(my_schema)\
.option("header", False)\
.load(filePath)
songsDF.printSchema()
songsDF.show()


[page 75]
df_select = songsDF.select("song_id", "song_length")

df_selectExpr = songsDF.selectExpr(
"concat('prefix-', song_id) as id ",
"cast(song_length as string) as length",
"TO_TIMESTAMP('11201926', 'MMyyyydd') as `timestamp`",
"1 as count"
)

from pyspark.sql.functions import expr, lit
df_select_expr = songsDF.select(
expr("concat('prefix-', song_id) as id"),
"song_length",
lit(1).alias("count")
)


[page 76]
from pyspark.sql.functions import col

songsDF.filter(col("song_length") != 246015)
songsDF.filter(songsDF["song_length"] != 246015)
songsDF.filter(songsDF.song_length != 246015)
songsDF.filter("song_length != 246015")
songsDF.filter("song_length <> 246015")

songsDF.where(col("song_length") != 246015)
songsDF.where(songsDF["song_length"] != 246015)
songsDF.where(songsDF.song_length != 246015)
songsDF.where("song_length != 246015")
songsDF.where("song_length <> 246015")


[page 77]
from pyspark.sql.functions import col

composer_filter = col("composer").isNotNull()
song_length_filter = col("song_length") < 120000
genre_ids_filter = col("genre_ids") == "465"
language_filter = col("language") == "3.0"

songs_bool = songsDF.withColumn(
"is_favor_songs",
composer_filter & song_length_filter & (language_filter | genre_ids_filter)
)\
.drop("song_id")\
.where("is_favor_songs")\
.limit(5)


[page 78]
from pyspark.sql.functions import udf, col

from pyspark.sql.types import StringType

def athemaster_str(element):
    return  "athemaster_" + element

udf_athemaster_str = udf(athemaster_str, StringType())
udf_df = songsDF.withColumn(
"athemaster_str",
udf_athemaster_str(col("artist_name"))
)
udf_df.show()



[page 79]
# drop null
df_drop_null_any = songsDF.na.drop(how="any")
df_drop_null_all = songsDF.na.drop(how="all")
df_drop_null_thresh = songsDF.na.drop(thresh=5)


df_drop_null_subset = songsDF.na.drop(subset=["composer", "lyricist"])


[page 80]
from pyspark.sql.functions import avg

# fill null
df_fill_null = songsDF.na.fill(300000, "song_length")
fill_dict = {
"composer": "no-composer",
"song_length": songsDF.select(avg("song_length")).first()[0]
}
df_fill_null_with_dict = songsDF.na.fill(fill_dict)

# replace
df_replace_null = songsDF.na.replace(["NoValue", "No-Value"], [None, None])


df_replace_null_subset = songsDF.na.replace(["NoValue", "No-Value"], [None, None], "composer")


[page 82-92]
from pyspark.sql.window import Window
from pyspark.sql.functions import rank
df_demo = spark.sql('select * from order_detail')
windowSpec = Window.partitionBy(df_demo.item)\
.orderBy(df_demo.quantity.desc())\
.rowsBetween(Window.unboundedPreceding, Window.currentRow)
df_demo_result = df_demo.withColumn('rank', rank().over(windowSpec))



[page 93-102]
from pyspark.sql.functions import avg
windowSpec = Window.partitionBy(df_demo.item)\
.orderBy(df_demo.date)\
.rowsBetween(-1, 0)
df_demo_result = df_demo.withColumn('MA', avg(df_demo.quantity).over(windowSpec))


[page 103]
from pyspark.sql.functions import sum as spark_sum, countDistinct
df_eg = df_demo.groupBy("date")\
.agg(spark_sum("quantity"), countDistinct("item"))\
.orderBy("date")


df_demo.createOrReplaceTempView("tb_demo")
df_tv = spark.sql("""
SELECT date, sum(quantity), count(distinct item)
FROM tb_demo
GROUP BY date
ORDER BY date
""")


[page 104]
df_no_null = df_demo.drop()
df_no_null.createOrReplaceTempView("tb_no_null")

df_gs_result = spark.sql("""
SELECT date, item, sum(quantity)
FROM tb_no_null
GROUP BY date, item
GROUPING SETS(date, item)
""")


[page 105]
# if you do not filter-out null values, you will get incorrect result.
df_no_null = df_demo.drop()
df_r_result = df_no_null.rollup("date", "item", "name").sum("quantity")



[page 106]
# if you do not filter-out null values, you will get incorrect result.
df_no_null = df_demo.drop()
df_c_result = df_no_null.cube("date", "item", "name").sum("quantity")


[page 107]
df_demo.groupBy("date")\
.pivot("item")\
.sum("quantity")\
.na.fill(0)\
.show()


[page 110]
df_demo.write.format("csv").option("header", True).save("/tmp/df_demo_han")
$ hdfs dfs -ls /tmp/df_demo_han

df_demo.write.format("parquet").mode("overwrite").saveAsTable("df_demo_han")
spark.sql("show tables").show()

"""

table_2_df = spark.sql(table_2_query)
